# ğŸ¦ Enterprise Batch Import Monitoring & Retry Dashboard

A data engineering demo project for ETL pipeline monitoring, with a modern dashboard that supports job retries, client-specific views, and interactive analytics. Built for fintech platforms that ingest and transform daily batch files from enterprise clients like banks and credit unions.

---

## ğŸ“Œ Project Overview

This project simulates a production-grade batch ETL platform with:

- Python-based ETL pipeline that ingests CSV batch files
- SQL database schema for tracking imports
- Streamlit dashboard for clients to:
  - Monitor job performance
  - Filter by time, status, duration
  - Retry failed jobs by uploading corrected files
  - Export reports to CSV/PDF

---

## ğŸ§± Architecture

+-----------------------+ +------------------+ +---------------------+
| Client Uploads | â€”> S3 | ETL Job | â€”> DB | Job History Table |
+-----------------------+ +------------------+ +---------------------+
|
v
+-------------------+
| Streamlit Dashboard|
+-------------------+


- **Backend**: Python ETL w/ helper modules for parsing and database loading
- **Database**: PostgreSQL (simulated with SQLAlchemy schema)
- **Frontend**: Streamlit dashboard for monitoring & retry logic
- **Demo**: Mock data simulates real job runs, including failures

---

## ğŸš€ Features

| Feature                             | Status |
|-------------------------------------|--------|
| Batch file ETL logic                | âœ… Done |
| SQL schema for job and account data | âœ… Done |
| Streamlit dashboard                 | âœ… Done |
| Login & client-specific filtering   | âœ… Done |
| Time range and job duration filters | âœ… Done |
| Export reports to CSV and PDF       | âœ… Done |
| Retry failed jobs via file upload   | âœ… Done |
| Mock API simulating backend data    | âœ… Done |

---

## ğŸ§ª Project Structure

project-root/
â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ batch_job.py # ETL orchestrator
â”‚ â”œâ”€â”€ db.py # DB models and session
â”‚ â”œâ”€â”€ parsers.py # Parsing logic
â”‚ â”œâ”€â”€ helpers.py # Shared functions
â”‚
â”œâ”€â”€ dashboard/
â”‚ â”œâ”€â”€ app.py # Streamlit dashboard app
â”‚ â”œâ”€â”€ mock_api.py # Simulated job data + retry logic
â”‚ â””â”€â”€ requirements.txt # Dashboard dependencies
â”‚
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ schema.sql # Raw SQL for table creation
â”œâ”€â”€ data/ # Sample batch files
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ§° Tech Stack

- **Python 3.9+**
- **Streamlit** â€“ interactive client dashboard
- **Pandas** â€“ data manipulation
- **Plotly** â€“ visualizations
- **FPDF** â€“ export reports to PDF
- **SQLAlchemy** â€“ DB schema and connection
- **PostgreSQL** â€“ target database

---

## ğŸ§ª Setup Instructions

1. **Clone the repo**

```bash
git clone https://github.com/your-username/batch-import-dashboard.git
cd batch-import-dashboard

2. ** Install Requirements **
```bash
pip install -r requirements.txt

3. ** Running the Dashboard **
```bash
streamlit run dashboard/app.py

Then visit http://localhost:8501 in your browser.
Login credentials
Username	Client View
bankcorp_admin	BankCorp
creditone_admin	CreditOne
fintrust_admin	FinTrust

Note: This is mock authentication for demo purposes only.

4. ** Reprocessing Failed Jobs **
Navigate to the "ğŸ› ï¸ Retry a Failed Job" section.

Select a failed job from the dropdown.

Upload a corrected CSV file.

Click "Reprocess Job" to simulate re-ingestion.

âœ… A success message appears if the mock reprocess function accepts the file.

5. ** Exporting Data **
Export filtered job history to CSV or PDF using the export buttons.

## Example Use Cases ##

Internal dashboard for support teams to track ETL job health

Client-facing portal to allow reprocessing and transparency

MVP for building a multi-tenant data observability tool

##  Future Improvements
âœ… Replace mock API with FastAPI or Flask backend

âœ… Connect to real PostgreSQL data source

ğŸ”’ Real authentication + RBAC

ğŸ“¨ Email/SMS alerts on job failures

ğŸ” Automated job retry logic (via Lambda or Celery)

ğŸ“¦ Dockerize for deployment

## Author
Built by an aspiring Data Engineer focused on scalable data ingestion, monitoring, and self-serve tolling for enterprise clients.

## License
MIT License. Free to use and adapt.